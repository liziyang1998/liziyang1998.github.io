{"pages":[{"title":"404","text":"","path":"404/index.html","date":"05-11","excerpt":""},{"title":"search","text":"","path":"search/index.html","date":"05-11","excerpt":""},{"title":"about","text":"李子旸中国科学技术大学 计算机科学与技术系 大三学生acm队员，2017、2018EC 银牌南京minieye 机器学习实习生 2019.7-2019.9华为上研所云核心网产品线 14级员工 2019.10 - ..github","path":"about/index.html","date":"05-11","excerpt":""}],"posts":[{"title":"迁移学习","text":"迁移学习概念迁移学习顾名思义就是把已经训练好的模型参数迁移到新的模型来帮助新的模型训练。考虑到大部分数据或任务是存在相关性的，所以通过迁移学习我们可以将已经学习到的模型参数通过某种方式分享给新模型从而加快并优化模型的学习效率而不用想大多数网络那样从头学习 比如我们要训练一个可以识别蚂蚁和蜜蜂的网络，我们可以在使用的imageNet数据集训练好的网络，迁移到我们使用的数据上来学习 Finetune实际应用中通常采用一种把已经训练好的模型的权值参数作为我们模型的初始化参数，称之为Finetune。迁移学习中的Finetune技术，本质上是让我们构建的新的模型，拥有一个较好的权值初始值 FInetune分为三步： 第一步：保存模型，或者拥有一个训练好的模型 第二步：加载模型，把训练好的模型的参数提取出来 第三步：初始化，将参数对应的值放入新模型 对于保存模型，官方文档有两种方法，一种是保存整个模型，另外一种是仅保存模型参数(推荐用法) 保存参数要用到state_dictpytorch中的state_dict是一个简单的字典对象，将每一层与对应参数建立映射关系，比如每一层的权值weight和偏置等 注意只有参数可训练的层才会保存到模型的state_dict中，优化器optimizer也有一个state_dict，包含了优化器的状态和被使用的超参数(lr, momentum)等 第一步：保存模型假设net=NET()，经过训练之后，通过torch.save保存1torch.save(net.state_dict(), PATH) 第二步：加载模型加载模型的参数1pretrained_state = torch.load(PATH) 第三步：初始化将取到的模型的参数对应的放到新模型中首先创建新模型：1net=Net() 获取新模型的参数state_dict1net_state_dict = net.state_dict() 将pretrained_dict中不属于net_state_dict的值删掉1pretrained_state_1 = &#123;k : v for k, v in pretrained_dict.items() if k in net_state_dict&#125; 然后更新新模型的参数1net_state_dict.update(pretrained_dict-1) 最后将更新的参数放回网络中1net.load_state_dict(net_state_dict) 只使用某几层方法是构建一个我们需要的网络，这个网络包含训练好的网络中的一部分，从这个网络里面提取出state_dict，然后再从训练好的网络把想要的参数对着填进去就得到了一个有初始参数的网络，同Finetune，然后load进新模型，自己添加的部分跟在后面，随机初始化权值，开始训练 例如Mobilenet结构12345678910111213141516171819202122232425262728293031323334353637383940414243444546class Net(nn.Module): def __init__(self): super(Net, self).__init__() def conv_bn(inp, oup, stride): return nn.Sequential( nn.Conv2d(inp, oup, 3, stride, 1, bias=False), nn.BatchNorm2d(oup), nn.ReLU(inplace=True) ) def conv_dw(inp, oup, stride): return nn.Sequential( nn.Conv2d(inp, inp, 3, stride, 1, groups=inp, bias=False), nn.BatchNorm2d(inp), nn.ReLU(inplace=True), nn.Conv2d(inp, oup, 1, 1, 0, bias=False), nn.BatchNorm2d(oup), nn.ReLU(inplace=True), ) self.model = nn.Sequential( conv_bn( 3, 32, 2), conv_dw( 32, 64, 1), conv_dw( 64, 128, 2), conv_dw(128, 128, 1), conv_dw(128, 256, 2), conv_dw(256, 256, 1), conv_dw(256, 512, 2), conv_dw(512, 512, 1), conv_dw(512, 512, 1), conv_dw(512, 512, 1), conv_dw(512, 512, 1), conv_dw(512, 512, 1), conv_dw(512, 1024, 2), conv_dw(1024, 1024, 1), nn.AvgPool2d(7), ) self.fc = nn.Linear(1024, 1000) def forward(self, x): x = self.model(x) x = x.view(-1, 1024) x = self.fc(x) return x 我们只需要前7层网络，缉拿个Sequential拆开123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657class Net(nn.Module): def __init__(self): super(Net, self).__init__() def conv_bn(inp, oup, stride): return nn.Sequential( nn.Conv2d(inp, oup, 3, stride, 1, bias=False), nn.BatchNorm2d(oup), nn.ReLU(inplace=True) ) def conv_dw(inp, oup, stride): return nn.Sequential( nn.Conv2d(inp, inp, 3, stride, 1, groups=inp, bias=False), nn.BatchNorm2d(inp), nn.ReLU(inplace=True), nn.Conv2d(inp, oup, 1, 1, 0, bias=False), nn.BatchNorm2d(oup), nn.ReLU(inplace=True), ) self.conv1 = conv_bn( 3, 32, 2) self.conv2 = conv_dw( 32, 64, 1) self.conv3 = conv_dw( 64, 128, 2) self.conv4 = conv_dw(128, 128, 1) self.conv5 = conv_dw(128, 256, 2) self.conv6 = conv_dw(256, 256, 1) self.conv7 = conv_dw(256, 512, 2) # 先面这些就不要了，只保留前7层 # 可以自己接后面的结构 ''' self.features = nn.Sequential( conv_dw(512, 512, 1), conv_dw(512, 512, 1), conv_dw(512, 512, 1), conv_dw(512, 512, 1), conv_dw(512, 512, 1), conv_dw(512, 1024, 2), conv_dw(1024, 1024, 1), nn.AvgPool2d(7),) self.fc = nn.Linear(1024, 1000) ''' def forward(self, x): x1 = self.conv1(x) x2 = self.conv2(x1) x3 = self.conv3(x2) x4 = self.conv4(x3) x5 = self.conv5(x4) x6 = self.conv6(x5) x7 = self.conv7(x6) #x8 = self.features(x7) #out = self.fc return (x1,x2,x3,x4,x4,x6,x7) 创建新模型的dict12net = Net()dict_new = net.state_dict() #这个就是包含前7层网路的字典，把预训练网络的参数对应填进去 冻结参数如果我们只需要更改最后几层，并且把前面的层都冻结，不再训练，那么就需要将参数的required_grad 设置为False 两种设置false的方法 第一种一种是在建立好网络之后主要思想是所有的模型都有一个函数model.children()，返回它的网络中的层，在每个层中，都有可以使用 .param()来获得的参数或权重。每个参数都有requires_grad的参数，默认情况下为True，所以冻结一个层，可以使用children返回这一层，并且将这层的参数的requires_grad设置为False 例：对前7层的参数冻结123456cnt = 0for child in model.children(): cnt= cnt + 1 if cnt &lt; 7: for param in child.parameters(): param.requires_grad=False 第二种第二种是在构建网络过程中123456789101112class Net(nn.Module): def __init__(self): super(Net, self).__init__() self.conv1 = nn.Conv2d(1, 6, 5) self.conv2 = nn.Conv2d(6, 16, 5) for p in self.parameters(): p.requires_grad=False self.fc1 = nn.Linear(16 * 5 * 5, 120) self.fc2 = nn.Linear(120, 84) self.fc3 = nn.Linear(84, 10) 这样在构建的过程中将梯度设为False,那么之前的层的梯度就都是False，后面的不变 注意设置完之后再传入optimizer的时候要根据有没有梯度过滤，不然还是被训练1optimizer.SGD(filter(lambda p : p.requires_grad, model.parameters()), lr=1e-4)","path":"2019/07/22/迁移学习/","date":"07-22","excerpt":"","tags":[{"name":"迁移学习","slug":"迁移学习","permalink":"http://www.lzycode.top/tags/迁移学习/"}],"preview":null},{"title":"torchvision","text":"torchvision持续更新中 数据集 datasets Datasets.ImageFolderDataSets.ImageFolder(root, transform=None, target_tranform=None,loader=default_loader)ImageFolder假设所有的数据按文件夹保存好，每个文件夹下面存储同一类别的图片，文件夹的名字为分类的名字 root:在指定的root目录下寻找图片 transform:对image进行转换操作 target_transform:对label进行变换 loader:指定加载图片的函数，默认是读取PIL image对象 变换 transformstransforms.Compose()将多个transforms组合 transforms.RandomResizedCrop()transforms.RandomResizedCrop(size, scale=(0.008, 1.0), ratio=(0.75, 1.333333), interpolation=2)随机大小，随机长度比裁剪原始图片，最后将图片resize到设定好的大小 size:输出图片的分辨率 scale:随机crop的大小区间，scale(0.08, 1.0)表示随机crop出来的图片会在0.08到1倍之间 ratio:随机长度比区间 interpolation:插值的方法，默认为双线性插值 transforms.RandomHorizontalFlip(p=0.5)以概率p对图片进行水平翻转，p默认为0.5 transforms.Normalize(mean, std)对数据按通道进行标准化，即先减去均值mean，再除以标准差std 模型 modelstorchvision.models()这个方法里面包含了alexnet、densenet、inception、resnet、squeezenet、vgg等常用的网络结构，并提供了预训练模型参数 1model = torchvision.models.resnet50(pretrained=True) 导入resnet50的预训练模型如果只需要网络结构，不需要参数1model = torchvision.models.resnet50(pretrained=False) pretrained 默认为False utils utils.make_gridtorchvision.utils.make_grid(tensor, nrow=8, padding=2, normalize=Flase, range=None, scale_each=False) 将batch个图片组成一张，nrow表示一行多少张图padding为2,则为图片先填充再连接，其中高度增加2*2=4，在横向上两张图之间只有一个padding，而不是两个","path":"2019/07/20/torchvision/","date":"07-20","excerpt":"","tags":[{"name":"torchvision","slug":"torchvision","permalink":"http://www.lzycode.top/tags/torchvision/"}],"preview":null},{"title":"torch","text":"torch持续更新中 数据处理部分utils.data.DataLoader()该接口主要用来将自定义的数据读取接口的输出或者Pytorch已有的数据读取接口的输入按照batch size 封装成Tensor，后续只需要再包装成Variable即可作为模型的输入 DataLoader类__init__函数中几个重要的输入 dataset，这个是pytorch已有的数据读取接口或者自定义的数据读取接口的输出 batch_size， shuffle，一般在训练数据中采用乱序 num_worker，几个进程来导入数据 当我们使用训练数据训练的时候，会使用__iter__方法，返回的是DataLoaderIter类 DataLoaderIter类__next__函数获取长度为batch_size的列表，然后将batch_size个tuple(每个tuple长度为2，第一个值是数据，Tensor类型，第二个值是标签，int类型)封装成一个list，这个list长度为2，两个值都是Tensor，第一个是batch_size个数据组成的Tensor，第二个是batch_size个标签组成的Tensor model部分model.train()和model.eval()model.train让model变成训练模式，此时dropout和batch normalization的操作在训练中起到防止网络过拟合的问题 model.eval会把BN和dropout固定住，用训练好的值，不然一旦test数据的batch_size过小，很容易就被BN层导致生成图片颜色失真极大 优化器optimtorch.optim使用torch.optim，构建一个optimizer对象，能保存当前的参数状态并且基于计算梯度更新参数 要构建一个优化器，必须给一个包含参数进行优化1optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9) SGD 随机梯度下降，最基础的优化方法，将数据拆分后再分批不断放入NN中计算，加速了NN的训练 Momentum 权重衰减传统的W的参数更新是将W的参数累加一个负的学习率乘以矫正值,Momentum的参数更新是借鉴动量的原理 optim.lr_scheduler该方法提供了多种基于epoch训练次数进行学习率调整的方法1optim.lr_scheduler.StepLR(optimizer, step_size, gamma=0.1) 当epoch每过step_size的时候，学习率变为初始的gamma倍 step通常有optimizer.step()和scheduler.step()两种 优化器optimizer的作用是根据网络反向传播的梯度信息来更新网络的参数，以起到降低loss的作用step函数使用的是参数空间的grad，也就是当前参数空间对应的梯度，即如果在step之前不清零，那么使用的这个grad会同上一个mini-batch相关，因为每个batch训练假定batch的大小就是训练集的大小，所以每训练一次更新一次参数，即optimizer.step()而scheduler.step()根据pytorch的定义是用来更新优化器的学习率的，所以一般按照epoch为单位进行更新，即多少个epoch更新一次学习率","path":"2019/07/20/torch/","date":"07-20","excerpt":"","tags":[{"name":"torch","slug":"torch","permalink":"http://www.lzycode.top/tags/torch/"}],"preview":null},{"title":"PWC-NET","text":"PWC-NET理解涉及知识 光流 CNN 图像上采样、下采样 上采样与反卷积 双线性插值 DenesNET 密集卷积网络 空洞卷积 整体框架PWC-NET的设计遵循简单和完善的原则： 特征金字塔形加工，翘曲和成本量使用 将图片投射进可学习的特征金字塔，PWC-NET网络使用当前光流估计来扭曲第二张图片学到的CNN特征，然后使用扭曲特征和第一章图片学到的CNN特征构建成本量，最后由CNN处理以估计光流 光流原理光流分为稀疏光流和稠密光流，PWC-NET或FlowNet得到的都是稠密光流 光流场就是连续图片序列中，从第t帧到第t+1帧，每个像素运动速度和运动方向 假设第t帧A点位置是(x1,y1)，在第t+1帧中A点位置是(x2,y2)，即$I_{t}(x1,y1) = I_{t+1}(x2,y2) = I_{t+1}(x1+u_{x},y1+u_{y})$，那么t-&gt;t+1的光流场为$(u_{x},u_{y})$，所以给定连续两帧图片，可以计算出光流场，分辨率和图片大小一致。 那么如果有光流场，就可以利用t+1帧的图片和光流场将t+1帧warp到t帧，得到t帧的图片，得到的坐标可能是浮点数，采用双线性插值。 光流可视化光流场是图片中每个像素都有一个x方向和y方向的位移，所以是一个分辨率和原图片一致的双通道图片。不同的颜色表示不同的运动方向，深浅表示运动的速度 将x和y转为极坐标，夹角$actan2(y,x)$代表方向，极径$\\sqrt{x^{2}+y^{2}}$代表位移大小，用下面的图片可以表示。对应上面的光流图片可以看出，红色的人是向右移动，蓝色是在向左上移动，并且人的头部运动要比身体速度更大 特征金字塔由6层卷积组成，通道数目从原图像的3通道变为16、32、64、96、128、196卷积层后跟进LeakyReLU层，参数设置为0.1123456789101112def conv(in_planes, out_planes, kernel_size=3, stride=1, padding=1, dilation=1): return nn.Sequential( nn.Conv2d(in_planes, out_planes, kernel_size=kernel_size, stride=stride, padding=padding, dilation=dilation, bias=True), nn.LeakyReLU(0.1))............self.conv1a = conv(3, 16, kernel_size=3, stride=2)self.conv1aa = conv(16, 16, kernel_size=3, stride=1)self.conv1b = conv(16, 16, kernel_size=3, stride=1) 通过2步长降低单特征图的大小对应后面的forward没有maxpool层 存疑 为什么不使用池化层？ 解答 信息丢失 翘曲层使用上一层上采样流(光流)与图二(t+1帧)结合，得出预测”图一”(t帧) 使用torch的grid_sample进行双线性插值： 双线性插值: 作用是对生成的图像与原图想重映射，由于坐标为实数，所以用插值法(对应可以使用其他映射方法，如果图像变化剧烈，或许可以使用lagrange插值或者分段插值？)在图像处理中，双线性插值使用非常多，在数学上，双线性插值可以看成是两个变量间的一维线性插值的延伸，对应图像的2维。 假设源图像大小是m*n,目标图像大小是a*b，那么边长之比就是m/a, n/b， 这个比例通常不是整数。目标图像第(i,j)个像素点，对应原图像为(i*m/a, j*n/b) 这个对应坐标一般不是整数，双线性插值通过寻找距离这个对应坐标最近的4各像素点，来计算该点的值 grid_sample函数 torch.nn.functional.grid_sample(input, grid) 给定输入和流场网格，使用网格中的输入像素位置计算输出 grid的值在[-1, 1]之间，这是标准化后的结果，[-1, -1]对应左上角，[1, 1]对应右下角，通过grid的值与目标图像像素位置找到源图像一个浮点数坐标，通过双线性插值确认出目标图像像素 12345678910111213# 生成网格B, C, H, W = x.size()xx = torch.arange(0, W).view(1,-1).repeat(H,1)yy = torch.arange(0, H).view(-1,1).repeat(1,W)xx = xx.view(1,1,H,W).repeat(B,1,1,1)yy = yy.view(1,1,H,W).repeat(B,1,1,1)grid = torch.cat((xx,yy),1).float()# 网格与上一层上采样流结合来扭曲图二，flo即为上一层的上采样流vgrid = Variable(grid) + flo # 得到扭曲后的图像output = nn.functional.grid_sample(x, vgrid) 存疑123456789mask = torch.autograd.Variable(torch.ones(x.size())).cuda()mask = nn.functional.grid_sample(mask, vgrid)mask[mask&lt;0.9999] = 0mask[mask&gt;0] = 1return output*mask# 功能上是将mask中大于0的变为1,0不变# 作用是？？ 成本量层使用特征构建一个成本量：匹配成本为提取的图像1特征与预测的”图像1”(来自翘曲层)的特征的相关性 相关量这个相关量是两张图的特征的整合，组合到一个更高的层次上。对于两张图像的特征表示，网络如何找到对应关系，即引入相关层，在两个特征映射之间执行乘法补丁比较 对于两张图$f_{1},f_{2}$的两块中心分别在x1和x2的patch的相关性为 c(x1,x2) = \\sum_{o\\epsilon[-k,k]\\times[-k,k]}\\langle f_{1}(x1+o),f_{2}(x2+o) \\rangle这样一块的大小就是$2K+1$。对于上面式子来说，计算方式与卷积操作一样，但没有过滤器。 如果每次计算一个点，那么一次计算量为$c\\times K^{2}$，总的计算量再乘$w^{2}\\times h^{2}$,相当于图一每个点和图二每个点都比较一次，计算量过于大，故引入限制比较的最大位移，对于图一每个位置x1，仅比较在$D:[2d+1]\\times [2d+1]$内的相关性$c(x1,x2)$ 具体操作使用correlation_package包来计算两张图的相关性输出通道为$(2*md +1)^2$其中md为maximun displacement设x1为图像1,x2为图像2计算公式： cv^{l}(x1,x2) = \\frac{1}{N} * (c^{l}_{1}(x1))^{T} * c^{l}_{w}(x2)T为转置，N为$c^{l}_{1}(x1)$列向量的长度对于一个L层的特征金字塔，我们仅需要计算具有有限范围的d像素的部分成本量，即$|x1-x2|_{\\infty} &lt; d$ 存疑 为什么仅需要计算d像素范围的部分成本量 x1-x2的无穷范数&lt;d，即两张图片的变换? 解答 输出的是$\\frac{1}{4}$分辨率的图片，并使用双线性插值得到原分辨率图片，所以仅需搜索d=4像素范围的成本量 论文同时对比了4层光流估算器和7层的，采用5层 光流估算器多层CNN，输入为成本量、图像1特征、上采样光流、上层特征输出为预测光流、对光流进行反卷积(上采样)，给下一层使用DenseNET DenseNET简介 CNN提高效果的方向，要么深，要么宽 深就面临着梯度消失的问题，ResNET解决了梯度消失的问题，DenseNET从feature入手，通过对feature的极致利用达到更好的效果和更少的参数 加深网络就会带来越来越明显的梯度消失的问题，解决方案核心都是从低层到高层之间建立直接连接，DenseNET(密集卷积网络)直接将所有层连接在一起 优点就是网络更窄，参数更少。输入信息和梯度信息在多层之间传递导致的梯度消失也会减少，因为是直接连接 DenseNET分成多个dense block，各个block之中feature的大小一致，而block之间的连接用来降低通道数量，通常为一个11卷积跟一个33卷积 图像上采样和下采样 缩小图像或称为下采样，主要目的为:使图像符合显示区域的大小，生成对应图像的缩略图 下采样原理：对于一副图像对于其进行s倍下采样，即将原始图像中s*s大小的图像变成一个像素，像素值为均值 放大图像或称为上采样，主要目的为:放大原图像，从而显示在更高分辨率的显示设备上。 上采样原理：图像放大几乎是采用内插值方法，采用合适的插值算法插入新的元素 上采样与反卷积 卷积可以通过3*3卷积核，将5*5的图像变成3*3的图片，反卷积即变回原尺寸，由于信息在卷积的过程中丢失，所以反卷积不是真实意义上的逆卷积 对于非分类神经网络例如图像分割网络，卷积后面的全连接层和softmax分类操作不再需要，那么如何反向传播计算loss，就需要上采样恢复到原大小 上采样通常使用插值法，反卷积可以通过网络学习参数 比如一张图像，进行5次卷积，分辨率缩小了2,4,8,16,32倍。对最后一层要进行32倍的上采样才能恢复到原大小才可以计算loss，这个上采样是通过反卷积实现的 最后一层32倍放大通常的不到精确的结果，细节无法恢复。于是将3、4层的输出也反卷积。 具体操作12345678910111213141516171819# 将上层(第6层)上采样流与图2特征结合 warp5 = self.warp(c25, up_flow6*0.625)# 扭曲到第一张图片里 corr5 = self.corr(c15, warp5) corr5 = self.leakyRELU(corr5)# 输入为成本量(corr5)，图像1特征(c15)，上采样光流(up_flow)，上层特征(up_feat)# 以下是DenseNET，每次卷积后将输入和输出连在一起，即保留了全部信息 x = torch.cat((corr5, c15, up_flow6, up_feat6), 1) x = torch.cat((self.conv5_0(x), x),1) x = torch.cat((self.conv5_1(x), x),1) x = torch.cat((self.conv5_2(x), x),1) x = torch.cat((self.conv5_3(x), x),1) x = torch.cat((self.conv5_4(x), x),1)# 预测流量 flow5 = self.predict_flow5(x)# 对预测流量上采样(反卷积) up_flow5 = self.deconv5(flow5)# 得到上层特征 up_feat5 = self.upfeat5(x) 值得注意一点是 12345678910def predict_flow(in_planes): return nn.Conv2d(in_planes,2,kernel_size=3,stride=1,padding=1,bias=True)# 反卷积def deconv(in_planes, out_planes, kernel_size=4, stride=2, padding=1): return nn.ConvTranspose2d(in_planes, out_planes, kernel_size, stride, padding, bias=True)self.predict_flow6 = predict_flow(od+dd[4])self.deconv6 = deconv(2, 2, kernel_size=4, stride=2, padding=1) self.upfeat6 = deconv(od+dd[4], 2, kernel_size=4, stride=2, padding=1) 可以看到预测光流函数输出是2通道，反卷积也是输出2通道，反卷特征也是输出2通细节道下层用到的地方在wrap函数，其中反卷积输出的预测光流要给grid添加偏置用来扭曲反卷特征也是2通道，我理解这是DenseNET的block连接，降低特征通道 一些细节从第6层开始，分辨率最低的一层，这一层与其他层操作不一样，先做一个corr12corr6 = self.corr(c16, c26) corr6 = self.leakyRELU(corr6) 然后卷积层得到预测光流，反卷积(上采样)后输出给下一层 第5~3层先对上一层的光流乘以系数，然后warp，之后corr，卷积，预测，反卷积，输出给下一层 第2层不再对预测的光流进行反卷积，而是进行下一步的上下文处理 关于系数：光流估算器建立在真实光流除以20的情况下，第6层到第5层，warp需要乘0.625，因为在第5层需要光流的时候，先乘20,然后这一层分辨率是输入的$\\frac{1}{32}$,所以$20 / 32 = 0.625$，其他层同理 上下文网络重新思考卷积 VGG网络的文章中，作者提出了关于卷积叠加的一个观察 7*7的卷积的正则等效与3个3*3卷积层的叠加。这样的设计不仅可以大幅度的减少参数，其本身带有正则性质的卷积图更容易学习，这就是绝大部分卷积的神经网络都在用小卷积核的原因 图像分割领域先对图像卷积再池化，降低图像尺寸增加感受野，但是图像分割要求卷积完后上采样进行预测，但是图像在先减小再增大的过程中，丢失了信息，所以通过设计空洞卷积解决问题 空洞卷积 空洞卷积即为在3*3卷积核中插入0的空洞，变成7*7的卷积核，但是有效元素还是只有9个。 虽然有效核为3*3，但是感受野已经增加到了7*7 更在两个1空洞卷积和2空洞卷积后面可以达到15*15的感受野，对比传统的卷积，3层3*3的卷积加起来，只能达到7*7的感受野 空洞卷积的好处就是不做池化丢失信息的情况下，增加了感受野，让每个卷积输出都包含较大的范围 具体操作123456789101112# 将通道减少，为了不丢失过多信息，采用空洞卷积的方式扩大接收野，同时减少通道self.dc_conv1 = conv(od+dd[4], 128, kernel_size=3, stride=1, padding=1, dilation=1)self.dc_conv2 = conv(128, 128, kernel_size=3, stride=1, padding=2, dilation=2)self.dc_conv3 = conv(128, 128, kernel_size=3, stride=1, padding=4, dilation=4)self.dc_conv4 = conv(128, 96, kernel_size=3, stride=1, padding=8, dilation=8)self.dc_conv5 = conv(96, 64, kernel_size=3, stride=1, padding=16, dilation=16)self.dc_conv6 = conv(64, 32, kernel_size=3, stride=1, padding=1, dilation=1)self.dc_conv7 = predict_flow(32)# 对于第2层输出的光流，减少通道，准备计算lossx = self.dc_conv4(self.dc_conv3(self.dc_conv2(self.dc_conv1(x))))flow2 = flow2 + self.dc_conv7(self.dc_conv6(self.dc_conv5(x))) 训练lossEPE (End-to-end point error):这是光流估计的标准误差测量，是所有像素点的ground truth与预测出来的光流之间的欧几里得距离的平均值 公式表示为： L(\\theta) = \\sum_{l=l0}^{L}\\alpha l\\sum_{x}|w_{\\theta}^{l}(x) - w_{GT}^{l}(x)|_{2} + \\gamma|\\theta|_{2}$|\\cdot|_{2}$代表向量的L2范数，第二项是用于规范模型的参数。微调之后使用更合适的训练loss函数： L(\\theta) = \\sum_{l=l0}^{L}\\alpha l\\sum_{x}(|w_{\\theta}^{l}(x) - w_{GT}^{l}(x)| + \\epsilon)^{q} + \\gamma|\\theta|_{2}$|\\cdot|$代表L1范数，q&lt;1，降低对异常值的惩罚，$\\epsilon$是一个小常数 具体实施其中系数$\\alpha_{6} = 0.32 $，$\\alpha_{5} = 0.08 $，$\\alpha_{4} = 0.02 $，$\\alpha_{3} = 0.01 $，$\\alpha_{2} = 0.005 $，$\\gamma$设置为0.0004，$q$设置为0.4，$\\epsilon$设置为0.01 将ground truth缩小20倍，然后对其进行下采样以获得各级别的光流$w_{GT}^{l}$ 训练成果MPI-Sintel数据集中clean的图片效果不如传统方法，但更适合于真实情况。原因在于clean图片中图像边缘和运动边界会对齐，传统方法就是使用图像边缘来定义运动边界 光流估算器通过删除DenseNet可以缩短运行时间，精确度下降5%，但是速度加快40%，删除上下文网络会导致训练集和验证集上出现更大的错误 光流估算器在每个级别使用5层CNN，使用4层或7层的结果见下图，结果表明越大容量的估算器性能越好 残差网络实现的光流估算器性能会有小的提升 翘曲在每层金字塔用预测的光流(增量)处理大的光流，去除翘曲层会导致精度的显著降低 成本量层中的搜索范围d，范围越大，误差越小，但是在d=2 的情况下已经可以处理输入分辨率200像素范围的运动","path":"2019/07/11/PWC-NET/","date":"07-11","excerpt":"","tags":[{"name":"PWC-NET","slug":"PWC-NET","permalink":"http://www.lzycode.top/tags/PWC-NET/"}],"preview":null},{"title":"近期需要整理","text":"大三期间人工智能 机器学习 优化问题并行计算 openmp mpi cuda 分布式系统实习期间深度学习 基础知识 CNNpytorch 用法PWC-NET 解析 复现成果 改进","path":"2019/07/10/近期需要整理/","date":"07-10","excerpt":"","tags":[{"name":"其他","slug":"其他","permalink":"http://www.lzycode.top/tags/其他/"}],"preview":null},{"title":"优化问题","text":"优化问题：min f(x)$f(x)$是目标函数，可以有限制条件$h(x) &lt; 0$,限制x的取值范围 优化问题形式化为搜索问题搜索问题是通用模型，优化问题可建模为搜索问题，","path":"2019/05/14/优化问题/","date":"05-14","excerpt":"","tags":[{"name":"优化问题","slug":"优化问题","permalink":"http://www.lzycode.top/tags/优化问题/"}],"preview":null},{"title":"华为2019CodeCraft","text":"华为2019软件精英挑战赛总结 以下是比赛总结，穿插干货 队伍战队名：Nebula战队成员：李子旸，曾明亮，任正行上合赛区 中国科学技术大学初赛全国第一，复赛上合第一，决赛全国4强 三等奖 初赛我们三个人都是大三，对于这个codecraft比赛之前全然不了解(….) 是我偶然从一个acm竞赛群里看到有人说华为软件精英挑战赛，记得当时说的是拿个赛区64强就可以免技术笔试，32强就可以免技术面试，然后我就开始招募队友打算水一水 今年变为4强才免技术面试，看来往年水分还是比较多的，后来找到一个同学和另一个acm队友，开始初赛的征程 刚发布下来初赛题目的时候，我们三个在lug室研读了一下午的题目要求，才刚刚搞明白整个调度流程，查了很多关于车辆调度的论文和算法，大致想去采用模拟退火，遗传、蚁群算法等 实际效果并不好，并且确定了要本地写一个判题器方便条参并且验证思路。 然而前期我们也比较水，不重视这个比赛，花了大概一周的时候去看这些东西并且找往年的参赛感悟和经验总结，看到去年普便出现取平均的效果比LSTM等算法要好，并且遗传等算法我们初步感觉也不太好写时间开销也可能很大也不好避免死锁的问题，就把这个思路放到一边去了。 后来我们回到了最一开始的思路 一辆一辆发， 对每辆车走最短路，然后分速度或者不分速度确定发车时间区间。比如分速度意为对速度为4的车发车区间随即打乱在[0, 1000]内，就是在1000个时间片内把所有速度为4的车平均发。用这个办法来避免死锁的发生。存在的问题就是需要大量试验并且存在运气成分。 然后就是关键的最短路的部分，我们采取的是伪最短路，或者说是道路拥塞控制，跟道路长度没有任何关系，一直到决赛都是用这个模型，权重取的是道路宽度的负比例，最大速度的负比例，以及该路上会通过的车辆总数的正比例，通过多次迭代静态生成所有车辆的路径。正比例和负比例的系数为需要调的参数，初赛模型中还有每种速度的发车区间需要调。通过这个模型我们初赛练习赛的时间从两张图3000多提高到最好500多。我们也开始真正认真起来对待这个比赛。 由于我们的模型需要大量的试验确定参数，所以我们一直在写判题器，不过直到比赛前一天才和官方对上，还是因为另一队发现了官方的漏洞我们才改对(吐槽一下发车规则，同一条路的两个方向，如果有一个方向堵住不能发车，另一个方向也不能发车。。。)。 接下来就是初赛的正赛环节，几个从来熬夜到2 3点甚至4 5点第二天12点多起的人，对于正赛9点开始有点难受。。。发布地图之前我们猜测可能会增加到20w，10w不过最后还是6w，并且一直延续到决赛，主办方还是挺手下留情的。 发布下来之后我们先跑了一遍地图一然后交了一遍对了以下判题器发现完全对上之后，便开始一整天的调参环节，中午的时候我们用这个模型大概调到了2100左右 由于比赛成绩只保留了最后100次，当时的成绩已不可考证，吃过饭继续调参，稍微改了改发车区间大概调到2000多，然后我们开始划水，最后三小时的时候我们及时发现了一个正负比例系数的问题，改了之后到1800多，正式结束初赛。 后来群里有人发起投票让大家投自己的成绩，数了下大概有10队2000以内的，我们当时虚的一批，不过成绩真正发布之后。都是骗子。。发现我们是全国第一之后，还是很激动的，从这开始我们开始决定跷课搞比赛。 复赛复赛增添了新的规则，增加了预置车辆和优先车辆。更容易死锁，所以我们初赛的模型变得非常被动，于是开始尝试解死锁，即改变第一辆等待车的方向，从原死锁方向变更到另一个方向，由于是一个NP问题，地图上每一辆车都是一个变量，每一次改变都会造成蝴蝶效应，不可预期，我们刚开始也不能确定会不会成功，后来实践证明，在道路负载不是很极端的情况下一般是可以解开死锁。 复赛发车我们没有考虑速度因素，把所有的车全部按照计划时间发。使用一个极限流量maxRunningCarAmount来控制道路总流量，即当前流量如果超过了maxRunningCarAmount，即把现在要发的车的时间延后。对于优先车，我们的方法是优先发优先车，让优先车尽快结束。 但是成绩不是非常好，两张图大概在5000左右，距离咕咕咕4300的理论极限还有很大差距。 这时候我们想了几个方法： 方法一把电势的思想引入到此模型中，对每个路口规定一个电势，非预置车辆只能从电势高(低)的地方走到电势低(高)的地方，只要保证预置车辆不死锁，那么此模型肯定不会死锁，且道路流量可以增大。构建电势采用类似蛇形矩阵的思想，最外围为比内围高，电势逐渐降低。后期试验证明道路流量可以急剧增大，总共6w多辆车在同一时间片可以达到接近2w辆车，但是由于每辆车路线距离边长且拥堵，最终结果差得很多，不过我们没有继续改下去，有可能存在通过速度不同发车会有更好的结果？ 方法二跟实际结合，猜测地图会越来越像城际道路(正是决赛的8张地图)。真实城市中有高架桥，类比模型中的高速路。让高速车尽量多的走高速路，但是实际效果不好，因为复赛练习赛的地图毫无这个特点。(因为这个思想比较早，所以我们决赛的时候也忘记了，后来才想起来曾经有过这个模型，感觉有点遗憾) 方法三我们的路径完全是静态生成的，只有解锁是又生成了一遍路径，所以采取动态规划的思想。具体思路与静态生成路径的思路大体一样。并且加入了新的解锁方法，深度解锁，即倒退若干个时间片重新规划当前发车的车的路径。实际效果两张图大概5600。 几个方法都没有原先的静态生成好，我们这时候陷入江局，后来决定中西结合 中药(动态规划)好，西药(静态生成)快，即把动态规划和静态生成合在一起，一部分车静态生成，一部分车动态规划。把80%的车静态生成，剩下的动态生成，并且同时采用浅层解锁 和 深度解锁，即更改第一辆等待车辆的方向浅层解锁，如果没有可以更改就倒退若干个时间片，重新规划。 实践证明我们的中西结合是正确的，最终在复赛练习赛中达到了4500，距离咕咕咕的理论极限只差200 期间还有一些小优化，比如最后的一千辆车采用最短路，剪掉尾巴。当优先车总数下降的时候，增大道路最大限制流量。 上海正式赛由于需要调的参数比较多，各种正负比例因子、道路最大流量、迭代次数、增大的道路最大流量、结尾采用最短路的车辆等等，我们想了想决定使用超算。同时跑30多次，上述参数从一个我们给定的范围内随即取得。由于踩了一些坑，导致我们正是赛前一天晚上才部署好超算，初步的结果证明有一些提高。 正赛当天需求变更，我们觉得时间不是很充裕，并且为了保证程序不出bug，所以不变更。但是正式赛的时候暴露了我们的一个问题，就是加入了浅层解锁和深度解锁之后，跑一遍大概要5、6分钟。对于新的地图还得要先找出来最优参数，才能放到超算里面进一步找更优的，但是我们跑一遍程序的时间太慢，所以迟迟没能开始地图二的测试，开始测试地图二的时候已经到了最后一个小时，通过短暂的测试，我们确定了一组参数，成绩是3086，交上去之后还剩半小时，我们这时候又测试出了一组2900多的，然后博弈了一下，决定交，然后就出问题了，最后30s的时候返回了失败，这时候我们极限操作返回了上一个版本，交了上去，交完之后马上就关掉了提交通道。 不过由于时间太紧张，我们并没有完全回到上一个版本，本地测试了一下大概是没有问题，但是有可能还有bug，事后证明是本地判题器有一个判断没加上，没有判断发车时间是否大于计划时间，也是因为采用了新地图，我们参数有变更，之前的地图不会有这种问题。 于是我们和一直以来都照顾我们的专家一起自闭了很久。然后帮我们打电话问了康康老师，最后的结果是3097，虽然不是最好成绩，但最起码没有因为这个问题gg掉，不然就真的太遗憾了。最后奖品是V20，正好我的手机用了两年该换了23333 决赛决赛增加了车牌识别，我们三个毫无cv和ai基础，开始了一周的现学。最终决定采用开源的车牌识别代码，魔改成比赛要用的。并且后续地图不可见，黒箱测试。 由于我们对华为云使用太过生疏，还有模型也迟迟不能移植成功，一直没有成绩。并且复赛暴露的问题是解锁花费的时间太久，并且参数过多，黒箱测试根本不行。于是我们两条路走，一边继续搭建ai，一边优化代码加快速度。 决赛代码的大体思想是继续沿用之前的模型，把复赛需求变成修改10%路径的加入进去，这样浅层解锁如果第一辆是预置车便也可以重新规划路径。如果10%没有用完，便把剩下的预置车修改为最短路。大量实验并结合理论思路，把复赛的大量参数减少，到最后只剩一个道路最大流量。并且对比复赛地图和决赛地图，我们发现了一个适合我们模型的特点，就是道路最大流量与最终结果大致呈现为一个二次函数，越规整的地图这个特点越明显。然后发现道路最大流量一般在道路总流量的15%-20%之间。于是我们准备对黒箱测试写自适应代码。我们从道路总流量的10%-30%之间选取几个点作为道路最大流量，测试结果，再选其中结果最小的那个，继续在结果最小的点附近找更小的点，最终返回一个15分钟内可以找到的最优解。于是优化代码成了当务之急。 其实非常感谢一下上海华为对我们的支持，因为我们迟迟没有成绩，那边很担心，就派专家过来帮助我们，派了一个算法专家一个ai专家。并且让我们在比赛前两天让我们科大的三个队到酒店集中培训 五星级酒店啊，真豪华啊，奢侈啊，每天大鱼大肉啊，最终还是有效果的，我们ai终于搭建完成，识别率上到98%，代码优化也降到了跑一次只需要30-40s，可以在15分钟内跑10+次，成绩也到了2595，大概排名第4 (有两个队一直不交完整代码，总是一张图一张图的测试，但还是被我发现了，他们的成绩更好一些，不过大家都是调参，最重要看的还是黒箱的自适应)。 一些小优化：复赛用到的最后提升道路最大流量我们更改成了，在优先车下降的时候道路最大流量 * 1.2 深圳正式赛加了一个抽签分组的环节，之前我们还吐槽说直接总排名取前几不就完了，后面成绩证明，真香。这里点明表扬队友的好手气，没有和那几支强队分到一起。 我们吸取了复赛差点gg的教训，决定最后一小时的时候交一次最后代码，对于需求变更我们也不再做修改。于是赛场上并没有什么波折，3个小时很快就过去了。(期间麻烦hr小姐姐去打探了咕咕咕，FatCat等队的成绩，发现他们贼猛。。但索性没有分到一组，只要我们一直保持小组第一就不会提前碰上，后面还真是一直到决赛才碰到他们) 接下来是深圳游记下午去了东莞的松山湖研究所，真的美啊，虽然下雨了，但是也在最合适的时间停了。最好的办法就是上图了 一个小插曲：吃完晚饭，有人说看自己ai模型调用次数就可以推出自己跑了几轮，我们马上登陆modelarts，看到是4800次，我们后来推算了很久大概是调用了600次，如果每张地图还是测试100张的话，我们就进前四了。 颁奖典礼颁奖典礼上峰峰老师复盘比赛，放出了所有地图，后续的地图全都是类城市地图，我们后悔了很久。。。因为这种地图对于我们的模型并没有任何优势，我们三个对于能否进8强开始渐渐失去信心。 又一个小插曲：在最后8进4，冠亚争夺赛的时候，峰峰老师放出的两种发车策略对比时，有一张的发车策略跟我们非常像 最终终于公布成绩了，我们拿到了季军，这50多天的时间我们没有白费，最终虽然有一些遗憾，但我们离咕咕咕确实还有差距。明年再战。 两个插曲后续，我们确实调用了600次，进前四了，发车策略对比中，比较差的那个确实是我们的发车策略，每个时间片都控制道路最大流量，并且预置车结束的时候提升流量，即乘1.2，其实正赛最后一小时的时候我提出把提升流量的时间提前，不是预置车结束的时刻，而是优先车下降的时刻，后续测试8张地图，都能提升不少，能一直稳住小组第一并且冠亚争夺赛能拿到第三，不过有遗憾也没办法了。 最后放一张合影，膜一下c位咕咕咕 最后收官晚宴，华为是真的大气，在海边酒店举行的海鲜自助，还有非常丰富的活动，还给我过了第三次生日，自己一次，上海华为一次，深圳总部一次23333333，蓝牙耳机好评++。第一次见到了海边，海风吹着真舒服。 今年有些小遗憾，明年继续来肝","path":"2019/05/13/华为2019CodeCraft/","date":"05-13","excerpt":"","tags":[{"name":"CodeCraft","slug":"CodeCraft","permalink":"http://www.lzycode.top/tags/CodeCraft/"}],"preview":"http://puisqm7fx.bkt.clouddn.com/CodeCraft.png"},{"title":"HelloWorld","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new \"My New Post\" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","path":"2019/05/10/helloworld/","date":"05-10","excerpt":"","tags":[],"preview":null}]}