{"pages":[{"title":"404","text":"","path":"404/index.html","date":"05-11","excerpt":""},{"title":"about","text":"李子旸中国科学技术大学 计算机科学与技术系 大三学生acm队员，2017、2018EC 银牌南京minieye 机器学习实习生 2019.7-2019.9华为上研所云核心网产品线 14级员工 2019.10 - ..github","path":"about/index.html","date":"05-11","excerpt":""},{"title":"search","text":"","path":"search/index.html","date":"05-11","excerpt":""}],"posts":[{"title":"PWC-NET","text":"PWC-NET理解涉及知识 CNN 图像上采样、下采样 上采样与反卷积 双线性插值 DenesNET 密集卷积网络 空洞卷积 整体框架PWC-NET的设计遵循简单和完善的原则： 特征金字塔形加工，翘曲和成本量使用 将图片投射进可学习的特征金字塔，PWC-NET网络使用当前光流估计来扭曲第二张图片学到的CNN特征，然后使用扭曲特征和第一章图片学到的CNN特征构建成本量，最后由CNN处理以估计光流 特征金字塔由6层卷积组成，通道数目从原图像的3通道变为16、32、64、96、128、196卷积层后跟进LeakyReLU层，参数设置为0.1 123456789101112def conv(in_planes, out_planes, kernel_size=3, stride=1, padding=1, dilation=1): return nn.Sequential( nn.Conv2d(in_planes, out_planes, kernel_size=kernel_size, stride=stride, padding=padding, dilation=dilation, bias=True), nn.LeakyReLU(0.1))............self.conv1a = conv(3, 16, kernel_size=3, stride=2)self.conv1aa = conv(16, 16, kernel_size=3, stride=1)self.conv1b = conv(16, 16, kernel_size=3, stride=1) 通过2步长降低单特征图的大小对应后面的forward没有maxpool层 存疑为什么不使用池化层？ 翘曲层使用上一层上采样流与图二特征结合，具体为使用双线形插值实现变形操作 使用torch的grid_sample进行双线性插值： 双线性插值: 作用是对生成的图像与原图想重映射，由于坐标为实数，所以用插值法(对应可以使用其他映射方法，如果图像变化剧烈，或许可以使用lagrange插值或者分段插值？)在图像处理中，双线性插值使用非常多，在数学上，双线性插值可以看成是两个变量间的一维线性插值的延伸，对应图像的2维。 假设源图像大小是m*n,目标图像大小是a*b，那么边长之比就是m/a, n/b， 这个比例通常不是整数。目标图像第(i,j)个像素点，对应原图像为(i*m/a, j*n/b) 这个对应坐标一般不是整数，双线性插值通过寻找距离这个对应坐标最近的4各像素点，来计算该点的值 grid_sample函数 torch.nn.functional.grid_sample(input, grid) 给定输入和流场网格，使用网格中的输入像素位置计算输出 grid的值在[-1, 1]之间，这是标准化后的结果，[-1, -1]对应左上角，[1, 1]对应右下角，通过grid的值与目标图像像素位置找到源图像一个浮点数坐标，通过双线性插值确认出目标图像像素 12345678910111213# 生成网格B, C, H, W = x.size()xx = torch.arange(0, W).view(1,-1).repeat(H,1)yy = torch.arange(0, H).view(-1,1).repeat(1,W)xx = xx.view(1,1,H,W).repeat(B,1,1,1)yy = yy.view(1,1,H,W).repeat(B,1,1,1)grid = torch.cat((xx,yy),1).float()# 网格与上一层上采样流结合来扭曲图二，flo即为上一层的上采样流vgrid = Variable(grid) + flo # 得到扭曲后的图像output = nn.functional.grid_sample(x, vgrid) 存疑123456789mask = torch.autograd.Variable(torch.ones(x.size())).cuda()mask = nn.functional.grid_sample(mask, vgrid)mask[mask&lt;0.9999] = 0mask[mask&gt;0] = 1return output*mask# 功能上是将mask中大于0的变为1,0不变# 作用是？？ 成本量层使用特征构建一个成本量：匹配成本为图像1特征与图像2扭曲特征的相关性使用correlation_package包来计算两张图的相关性输出通道为$(2*md +1)^2$其中md为maximun displacement设x1为图像1,x2为图像2计算公式： cv^{l}(x1,x2) = \\frac{1}{N} * (c^{l}_{1}(x1))^{T} * c^{l}_{w}(x2)T为转置，N为$c^{l}_{1}(x1)$列向量的长度对于一个L层的特征金字塔，我们仅需要计算具有有限范围的d像素的部分成本量，即$|x1-x2|_{\\infty} &lt; d$ 存疑 为什么仅需要计算d像素范围的部分成本量 x1-x2的无穷范数&lt;d，即两张图片的变换? 光流估算器多层CNN，输入为成本量、图像1特征、上采样光流、上层特征输出为预测光流、对光流进行反卷积(上采样)，给下一层使用DenseNET DenseNET简介 CNN提高效果的方向，要么深，要么宽 深就面临着梯度消失的问题，ResNET解决了梯度消失的问题，DenseNET从feature入手，通过对feature的极致利用达到更好的效果和更少的参数 加深网络就会带来越来越明显的梯度消失的问题，解决方案核心都是从低层到高层之间建立直接连接，DenseNET(密集卷积网络)直接将所有层连接在一起 优点就是网络更窄，参数更少。输入信息和梯度信息在多层之间传递导致的梯度消失也会减少，因为是直接连接 DenseNET分成多个dense block，各个block之中feature的大小一致，而block之间的连接用来降低通道数量，通常为一个11卷积跟一个33卷积 图像上采样和下采样 缩小图像或称为下采样，主要目的为:使图像符合显示区域的大小，生成对应图像的缩略图 下采样原理：对于一副图像对于其进行s倍下采样，即将原始图像中s*s大小的图像变成一个像素，像素值为均值 放大图像或称为上采样，主要目的为:放大原图像，从而显示在更高分辨率的显示设备上。 上采样原理：图像放大几乎是采用内插值方法，采用合适的插值算法插入新的元素 上采样与反卷积 卷积可以通过3*3卷积核，将5*5的图像变成3*3的图片，反卷积即变回原尺寸，由于信息在卷积的过程中丢失，所以反卷积不是真实意义上的逆卷积 对于非分类神经网络例如图像分割网络，卷积后面的全连接层和softmax分类操作不再需要，那么如何反向传播计算loss，就需要上采样恢复到原大小 上采样通常使用插值法，反卷积可以通过网络学习参数 比如一张图像，进行5次卷积，分辨率缩小了2,4,8,16,32倍。对最后一层要进行32倍的上采样才能恢复到原大小才可以计算loss，这个上采样是通过反卷积实现的 最后一层32倍放大通常的不到精确的结果，细节无法恢复。于是将3、4层的输出也反卷积。 具体操作12345678910111213141516171819# 将上层(第6层)上采样流与图2特征结合 warp5 = self.warp(c25, up_flow6*0.625)# 扭曲到第一张图片里 corr5 = self.corr(c15, warp5) corr5 = self.leakyRELU(corr5)# 输入为成本量(corr5)，图像1特征(c15)，上采样光流(up_flow)，上层特征(up_feat)# 以下是DenseNET，每次卷积后将输入和输出连在一起，即保留了全部信息 x = torch.cat((corr5, c15, up_flow6, up_feat6), 1) x = torch.cat((self.conv5_0(x), x),1) x = torch.cat((self.conv5_1(x), x),1) x = torch.cat((self.conv5_2(x), x),1) x = torch.cat((self.conv5_3(x), x),1) x = torch.cat((self.conv5_4(x), x),1)# 预测流量 flow5 = self.predict_flow5(x)# 对预测流量上采样(反卷积) up_flow5 = self.deconv5(flow5)# 得到上层特征 up_feat5 = self.upfeat5(x) 值得注意一点是 12345678910def predict_flow(in_planes): return nn.Conv2d(in_planes,2,kernel_size=3,stride=1,padding=1,bias=True)# 反卷积def deconv(in_planes, out_planes, kernel_size=4, stride=2, padding=1): return nn.ConvTranspose2d(in_planes, out_planes, kernel_size, stride, padding, bias=True)self.predict_flow6 = predict_flow(od+dd[4])self.deconv6 = deconv(2, 2, kernel_size=4, stride=2, padding=1) self.upfeat6 = deconv(od+dd[4], 2, kernel_size=4, stride=2, padding=1) 可以看到预测光流函数输出是2通道，反卷积也是输出2通道，反卷特征也是输出2通细节道下层用到的地方在wrap函数，其中反卷积输出的预测光流要给grid添加偏置用来扭曲反卷特征也是2通道，我理解这是DenseNET的block连接，降低特征通道 一些细节从第6层开始，分辨率最低的一层，这一层与其他层操作不一样，先做一个corr 12corr6 = self.corr(c16, c26) corr6 = self.leakyRELU(corr6) 然后卷积层得到预测光流，反卷积(上采样)后输出给下一层 第5~3层先对上一层的光流乘以系数，然后warp，之后corr，卷积，预测，反卷积，输出给下一层 第2层不再对预测的光流进行反卷积，而是进行下一步的上下文处理 关于系数：光流估算器建立在真实光流除以20的情况下，第6层到第5层，warp需要乘0.625，因为在第5层需要光流的时候，先乘20,然后这一层分辨率是输入的$\\frac{1}{32}$,所以$20 / 32 = 0.625$，其他层同理 存疑通道数目如何确认？ 上下文网络重新思考卷积 VGG网络的文章中，作者提出了关于卷积叠加的一个观察 7*7的卷积的正则等效与3个3*3卷积层的叠加。这样的设计不仅可以大幅度的减少参数，其本身带有正则性质的卷积图更容易学习，这就是绝大部分卷积的神经网络都在用小卷积核的原因 图像分割领域先对图像卷积再池化，降低图像尺寸增加感受野，但是图像分割要求卷积完后上采样进行预测，但是图像在先减小再增大的过程中，丢失了信息，所以通过设计空洞卷积解决问题 空洞卷积 空洞卷积即为在3*3卷积核中插入0的空洞，变成7*7的卷积核，但是有效元素还是只有9个。 虽然有效核为3*3，但是感受野已经增加到了7*7 更在两个1空洞卷积和2空洞卷积后面可以达到15*15的感受野，对比传统的卷积，3层3*3的卷积加起来，只能达到7*7的感受野 空洞卷积的好处就是不做池化丢失信息的情况下，增加了感受野，让每个卷积输出都包含较大的范围 具体操作123456789101112# 将通道减少，恢复原分辨率self.dc_conv1 = conv(od+dd[4], 128, kernel_size=3, stride=1, padding=1, dilation=1)self.dc_conv2 = conv(128, 128, kernel_size=3, stride=1, padding=2, dilation=2)self.dc_conv3 = conv(128, 128, kernel_size=3, stride=1, padding=4, dilation=4)self.dc_conv4 = conv(128, 96, kernel_size=3, stride=1, padding=8, dilation=8)self.dc_conv5 = conv(96, 64, kernel_size=3, stride=1, padding=16, dilation=16)self.dc_conv6 = conv(64, 32, kernel_size=3, stride=1, padding=1, dilation=1)self.dc_conv7 = predict_flow(32)# 对于第2层输出的预测光流，恢复到原图像分辨率，准备计算lossx = self.dc_conv4(self.dc_conv3(self.dc_conv2(self.dc_conv1(x))))flow2 = flow2 + self.dc_conv7(self.dc_conv6(self.dc_conv5(x))) 训练loss","path":"2019/07/11/PWC-NET/","date":"07-11","excerpt":"","tags":[{"name":"PWC-NET","slug":"PWC-NET","permalink":"http://www.lzycode.top/tags/PWC-NET/"}],"preview":null},{"title":"近期需要整理","text":"大三期间人工智能 机器学习 优化问题并行计算 openmp mpi cuda 分布式系统实习期间深度学习 基础知识 CNNpytorch 用法PWC-NET 解析 复现成果 改进","path":"2019/07/10/近期需要整理/","date":"07-10","excerpt":"","tags":[{"name":"其他","slug":"其他","permalink":"http://www.lzycode.top/tags/其他/"}],"preview":null},{"title":"优化问题","text":"优化问题：min f(x)$f(x)$是目标函数，可以有限制条件$h(x) &lt; 0$,限制x的取值范围 优化问题形式化为搜索问题搜索问题是通用模型，优化问题可建模为搜索问题，","path":"2019/05/14/优化问题/","date":"05-14","excerpt":"","tags":[{"name":"优化问题","slug":"优化问题","permalink":"http://www.lzycode.top/tags/优化问题/"}],"preview":null},{"title":"华为2019CodeCraft","text":"华为2019软件精英挑战赛总结 以下是比赛总结，穿插干货 队伍战队名：Nebula战队成员：李子旸，曾明亮，任正行上合赛区 中国科学技术大学初赛全国第一，复赛上合第一，决赛全国4强 三等奖 初赛我们三个人都是大三，对于这个codecraft比赛之前全然不了解(….) 是我偶然从一个acm竞赛群里看到有人说华为软件精英挑战赛，记得当时说的是拿个赛区64强就可以免技术笔试，32强就可以免技术面试，然后我就开始招募队友打算水一水 今年变为4强才免技术面试，看来往年水分还是比较多的，后来找到一个同学和另一个acm队友，开始初赛的征程 刚发布下来初赛题目的时候，我们三个在lug室研读了一下午的题目要求，才刚刚搞明白整个调度流程，查了很多关于车辆调度的论文和算法，大致想去采用模拟退火，遗传、蚁群算法等 实际效果并不好，并且确定了要本地写一个判题器方便条参并且验证思路。 然而前期我们也比较水，不重视这个比赛，花了大概一周的时候去看这些东西并且找往年的参赛感悟和经验总结，看到去年普便出现取平均的效果比LSTM等算法要好，并且遗传等算法我们初步感觉也不太好写时间开销也可能很大也不好避免死锁的问题，就把这个思路放到一边去了。 后来我们回到了最一开始的思路 一辆一辆发， 对每辆车走最短路，然后分速度或者不分速度确定发车时间区间。比如分速度意为对速度为4的车发车区间随即打乱在[0, 1000]内，就是在1000个时间片内把所有速度为4的车平均发。用这个办法来避免死锁的发生。存在的问题就是需要大量试验并且存在运气成分。 然后就是关键的最短路的部分，我们采取的是伪最短路，或者说是道路拥塞控制，跟道路长度没有任何关系，一直到决赛都是用这个模型，权重取的是道路宽度的负比例，最大速度的负比例，以及该路上会通过的车辆总数的正比例，通过多次迭代静态生成所有车辆的路径。正比例和负比例的系数为需要调的参数，初赛模型中还有每种速度的发车区间需要调。通过这个模型我们初赛练习赛的时间从两张图3000多提高到最好500多。我们也开始真正认真起来对待这个比赛。 由于我们的模型需要大量的试验确定参数，所以我们一直在写判题器，不过直到比赛前一天才和官方对上，还是因为另一队发现了官方的漏洞我们才改对(吐槽一下发车规则，同一条路的两个方向，如果有一个方向堵住不能发车，另一个方向也不能发车。。。)。 接下来就是初赛的正赛环节，几个从来熬夜到2 3点甚至4 5点第二天12点多起的人，对于正赛9点开始有点难受。。。发布地图之前我们猜测可能会增加到20w，10w不过最后还是6w，并且一直延续到决赛，主办方还是挺手下留情的。 发布下来之后我们先跑了一遍地图一然后交了一遍对了以下判题器发现完全对上之后，便开始一整天的调参环节，中午的时候我们用这个模型大概调到了2100左右 由于比赛成绩只保留了最后100次，当时的成绩已不可考证，吃过饭继续调参，稍微改了改发车区间大概调到2000多，然后我们开始划水，最后三小时的时候我们及时发现了一个正负比例系数的问题，改了之后到1800多，正式结束初赛。 后来群里有人发起投票让大家投自己的成绩，数了下大概有10队2000以内的，我们当时虚的一批，不过成绩真正发布之后。都是骗子。。发现我们是全国第一之后，还是很激动的，从这开始我们开始决定跷课搞比赛。 复赛复赛增添了新的规则，增加了预置车辆和优先车辆。更容易死锁，所以我们初赛的模型变得非常被动，于是开始尝试解死锁，即改变第一辆等待车的方向，从原死锁方向变更到另一个方向，由于是一个NP问题，地图上每一辆车都是一个变量，每一次改变都会造成蝴蝶效应，不可预期，我们刚开始也不能确定会不会成功，后来实践证明，在道路负载不是很极端的情况下一般是可以解开死锁。 复赛发车我们没有考虑速度因素，把所有的车全部按照计划时间发。使用一个极限流量maxRunningCarAmount来控制道路总流量，即当前流量如果超过了maxRunningCarAmount，即把现在要发的车的时间延后。对于优先车，我们的方法是优先发优先车，让优先车尽快结束。 但是成绩不是非常好，两张图大概在5000左右，距离咕咕咕4300的理论极限还有很大差距。 这时候我们想了几个方法： 方法一把电势的思想引入到此模型中，对每个路口规定一个电势，非预置车辆只能从电势高(低)的地方走到电势低(高)的地方，只要保证预置车辆不死锁，那么此模型肯定不会死锁，且道路流量可以增大。构建电势采用类似蛇形矩阵的思想，最外围为比内围高，电势逐渐降低。后期试验证明道路流量可以急剧增大，总共6w多辆车在同一时间片可以达到接近2w辆车，但是由于每辆车路线距离边长且拥堵，最终结果差得很多，不过我们没有继续改下去，有可能存在通过速度不同发车会有更好的结果？ 方法二跟实际结合，猜测地图会越来越像城际道路(正是决赛的8张地图)。真实城市中有高架桥，类比模型中的高速路。让高速车尽量多的走高速路，但是实际效果不好，因为复赛练习赛的地图毫无这个特点。(因为这个思想比较早，所以我们决赛的时候也忘记了，后来才想起来曾经有过这个模型，感觉有点遗憾) 方法三我们的路径完全是静态生成的，只有解锁是又生成了一遍路径，所以采取动态规划的思想。具体思路与静态生成路径的思路大体一样。并且加入了新的解锁方法，深度解锁，即倒退若干个时间片重新规划当前发车的车的路径。实际效果两张图大概5600。 几个方法都没有原先的静态生成好，我们这时候陷入江局，后来决定中西结合 中药(动态规划)好，西药(静态生成)快，即把动态规划和静态生成合在一起，一部分车静态生成，一部分车动态规划。把80%的车静态生成，剩下的动态生成，并且同时采用浅层解锁 和 深度解锁，即更改第一辆等待车辆的方向浅层解锁，如果没有可以更改就倒退若干个时间片，重新规划。 实践证明我们的中西结合是正确的，最终在复赛练习赛中达到了4500，距离咕咕咕的理论极限只差200 期间还有一些小优化，比如最后的一千辆车采用最短路，剪掉尾巴。当优先车总数下降的时候，增大道路最大限制流量。 上海正式赛由于需要调的参数比较多，各种正负比例因子、道路最大流量、迭代次数、增大的道路最大流量、结尾采用最短路的车辆等等，我们想了想决定使用超算。同时跑30多次，上述参数从一个我们给定的范围内随即取得。由于踩了一些坑，导致我们正是赛前一天晚上才部署好超算，初步的结果证明有一些提高。 正赛当天需求变更，我们觉得时间不是很充裕，并且为了保证程序不出bug，所以不变更。但是正式赛的时候暴露了我们的一个问题，就是加入了浅层解锁和深度解锁之后，跑一遍大概要5、6分钟。对于新的地图还得要先找出来最优参数，才能放到超算里面进一步找更优的，但是我们跑一遍程序的时间太慢，所以迟迟没能开始地图二的测试，开始测试地图二的时候已经到了最后一个小时，通过短暂的测试，我们确定了一组参数，成绩是3086，交上去之后还剩半小时，我们这时候又测试出了一组2900多的，然后博弈了一下，决定交，然后就出问题了，最后30s的时候返回了失败，这时候我们极限操作返回了上一个版本，交了上去，交完之后马上就关掉了提交通道。 不过由于时间太紧张，我们并没有完全回到上一个版本，本地测试了一下大概是没有问题，但是有可能还有bug，事后证明是本地判题器有一个判断没加上，没有判断发车时间是否大于计划时间，也是因为采用了新地图，我们参数有变更，之前的地图不会有这种问题。 于是我们和一直以来都照顾我们的专家一起自闭了很久。然后帮我们打电话问了康康老师，最后的结果是3097，虽然不是最好成绩，但最起码没有因为这个问题gg掉，不然就真的太遗憾了。最后奖品是V20，正好我的手机用了两年该换了23333 决赛决赛增加了车牌识别，我们三个毫无cv和ai基础，开始了一周的现学。最终决定采用开源的车牌识别代码，魔改成比赛要用的。并且后续地图不可见，黒箱测试。 由于我们对华为云使用太过生疏，还有模型也迟迟不能移植成功，一直没有成绩。并且复赛暴露的问题是解锁花费的时间太久，并且参数过多，黒箱测试根本不行。于是我们两条路走，一边继续搭建ai，一边优化代码加快速度。 决赛代码的大体思想是继续沿用之前的模型，把复赛需求变成修改10%路径的加入进去，这样浅层解锁如果第一辆是预置车便也可以重新规划路径。如果10%没有用完，便把剩下的预置车修改为最短路。大量实验并结合理论思路，把复赛的大量参数减少，到最后只剩一个道路最大流量。并且对比复赛地图和决赛地图，我们发现了一个适合我们模型的特点，就是道路最大流量与最终结果大致呈现为一个二次函数，越规整的地图这个特点越明显。然后发现道路最大流量一般在道路总流量的15%-20%之间。于是我们准备对黒箱测试写自适应代码。我们从道路总流量的10%-30%之间选取几个点作为道路最大流量，测试结果，再选其中结果最小的那个，继续在结果最小的点附近找更小的点，最终返回一个15分钟内可以找到的最优解。于是优化代码成了当务之急。 其实非常感谢一下上海华为对我们的支持，因为我们迟迟没有成绩，那边很担心，就派专家过来帮助我们，派了一个算法专家一个ai专家。并且让我们在比赛前两天让我们科大的三个队到酒店集中培训 五星级酒店啊，真豪华啊，奢侈啊，每天大鱼大肉啊，最终还是有效果的，我们ai终于搭建完成，识别率上到98%，代码优化也降到了跑一次只需要30-40s，可以在15分钟内跑10+次，成绩也到了2595，大概排名第4 (有两个队一直不交完整代码，总是一张图一张图的测试，但还是被我发现了，他们的成绩更好一些，不过大家都是调参，最重要看的还是黒箱的自适应)。 一些小优化：复赛用到的最后提升道路最大流量我们更改成了，在优先车下降的时候道路最大流量 * 1.2 深圳正式赛加了一个抽签分组的环节，之前我们还吐槽说直接总排名取前几不就完了，后面成绩证明，真香。这里点明表扬队友的好手气，没有和那几支强队分到一起。 我们吸取了复赛差点gg的教训，决定最后一小时的时候交一次最后代码，对于需求变更我们也不再做修改。于是赛场上并没有什么波折，3个小时很快就过去了。(期间麻烦hr小姐姐去打探了咕咕咕，FatCat等队的成绩，发现他们贼猛。。但索性没有分到一组，只要我们一直保持小组第一就不会提前碰上，后面还真是一直到决赛才碰到他们) 接下来是深圳游记下午去了东莞的松山湖研究所，真的美啊，虽然下雨了，但是也在最合适的时间停了。最好的办法就是上图了 一个小插曲：吃完晚饭，有人说看自己ai模型调用次数就可以推出自己跑了几轮，我们马上登陆modelarts，看到是4800次，我们后来推算了很久大概是调用了600次，如果每张地图还是测试100张的话，我们就进前四了。 颁奖典礼颁奖典礼上峰峰老师复盘比赛，放出了所有地图，后续的地图全都是类城市地图，我们后悔了很久。。。因为这种地图对于我们的模型并没有任何优势，我们三个对于能否进8强开始渐渐失去信心。 又一个小插曲：在最后8进4，冠亚争夺赛的时候，峰峰老师放出的两种发车策略对比时，有一张的发车策略跟我们非常像 最终终于公布成绩了，我们拿到了季军，这50多天的时间我们没有白费，最终虽然有一些遗憾，但我们离咕咕咕确实还有差距。明年再战。 两个插曲后续，我们确实调用了600次，进前四了，发车策略对比中，比较差的那个确实是我们的发车策略，每个时间片都控制道路最大流量，并且预置车结束的时候提升流量，即乘1.2，其实正赛最后一小时的时候我提出把提升流量的时间提前，不是预置车结束的时刻，而是优先车下降的时刻，后续测试8张地图，都能提升不少，能一直稳住小组第一并且冠亚争夺赛能拿到第三，不过有遗憾也没办法了。 最后放一张合影，膜一下c位咕咕咕 最后收官晚宴，华为是真的大气，在海边酒店举行的海鲜自助，还有非常丰富的活动，还给我过了第三次生日，自己一次，上海华为一次，深圳总部一次23333333，蓝牙耳机好评++。第一次见到了海边，海风吹着真舒服。 今年有些小遗憾，明年继续来肝","path":"2019/05/13/华为2019CodeCraft/","date":"05-13","excerpt":"","tags":[{"name":"CodeCraft","slug":"CodeCraft","permalink":"http://www.lzycode.top/tags/CodeCraft/"}],"preview":"http://puisqm7fx.bkt.clouddn.com/CodeCraft.png"},{"title":"HelloWorld","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new \"My New Post\" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","path":"2019/05/10/helloworld/","date":"05-10","excerpt":"","tags":[],"preview":null}]}